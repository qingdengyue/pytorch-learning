{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:tensor([1., 1., 1., 1., 1.])\n",
      "y:tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.ones(5) #input\n",
    "print(f\"x:{x}\")\n",
    "y=torch.zeros(3) #output\n",
    "print(f\"y:{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:tensor([[-0.7670, -1.1462, -0.9235],\n",
      "        [ 0.4024, -0.4532, -0.3445],\n",
      "        [ 1.2219,  1.0551, -0.3843],\n",
      "        [ 0.3173,  0.4211,  1.4122],\n",
      "        [-1.3351,  0.3518,  1.5614]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.randn(5,3,requires_grad=True)\n",
    "print(f\"w:{w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:tensor([ 0.8067, -1.0687,  0.7033], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b=torch.randn(3,requires_grad=True)\n",
    "print(f\"b:{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:tensor([ 0.6462, -0.8402,  2.0246], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=torch.matmul(x,w)+b\n",
    "print(f\"z:{z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1917, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss=torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flow](./images/comp-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z=<AddBackward0 object at 0x000002642E451720>\n",
      "Gradient function for loss=<BinaryCrossEntropyWithLogitsBackward0 object at 0x000002641756F880>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z={z.grad_fn}\")\n",
    "print(f\"Gradient function for loss={loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2187, 0.1005, 0.2945],\n",
      "        [0.2187, 0.1005, 0.2945],\n",
      "        [0.2187, 0.1005, 0.2945],\n",
      "        [0.2187, 0.1005, 0.2945],\n",
      "        [0.2187, 0.1005, 0.2945]])\n",
      "tensor([0.2187, 0.1005, 0.2945])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
